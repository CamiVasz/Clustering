\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{url}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Credit card fraud detection using a clustering approach\\
}

\author{\IEEEauthorblockN{María Camila Vásquez Correa}
\IEEEauthorblockA{\textit{Mathematics department} \\
\textit{Universidad EAFIT}\\
Medellín, Colombia \\
mvasqu49@eafit.edu.co}
}

\maketitle

\begin{abstract}
In this article is presented a comparison of 5 clustering algorithms to segregate data on credit card fraud detection, these algorithms are: k-means, an extension to k-means, fuzzy c-means, subtractive and mountain clustering. These algorithms are first evaluated in the iris dataset, and finally in our case study. Several metrics for finding the clusters are compared and a variation of parameters is performed.
\end{abstract}

\begin{IEEEkeywords}
Cluster, k-means, credit card, fraud.
\end{IEEEkeywords}

\section{Introduction}
Given the current global economic
context, increasing efforts are being made to both prevent and detect fraud. Credit card financial products can derive in unsecured and unplanned credit card risks and should not be underestimated. Stopping credit card fraud has become a hot issue in academia and industry. In the field of credit card fraud, the mistake of letting go of fraudulent transactions is much more expensive than mistakenly intercepting normal transactions, and the number of fraudulent transactions is far less than the normal number of transactions. \\
Clustering, as unsupervised data mining technique, deals with the problem of dividing a given set of entities into meaningful subsets. Clusters resulted from this data segmentation are required to be to be homogeneous and/or well separated, entities within the same group being similar while entities within different groups being dissimilar. 

\section{Literature review}
A survey for the methods on fraud detection via clustering is presented in\cite{Sabau2012}. This survey takes into account the algorithm used and the type of fraud the models are attacking. Is clear that the prevalent method is the k-means, however, some other types of clustering including graph based, hierarchical and density based algorithms were also implemented in the reviewed works. \\
As an example, some of the methods that make good use of the k-means clustering are: \cite{Wang2019a}, who compares the performance of Support Vector Machines along with clustering algorithms and other classification approaches, and finds put that the former is better at handling highly unbalanced data. \cite{Santos2018} uses a Naive Bayes clustering approach (as well as \cite{Carneiro2015a}) along with k-means clustering. In these algorithms, there is a clear pattern of hybrid methodologies, some other examples include \cite{Sathyapriya2019}, \cite{Kumari2017}, and \cite{Fashoto2016a} that use Hidden Markov models along with k-means clustering with the objective of reducing the false alarms produced by other algorithms of fraud detection and to include information about past transactions to build a costumer profile. However, some authors like \cite{Carneiro2015a} use only Naive Bayes clustering along with a Multi Layer perceptron to build a clustering engine and \cite{Behera2015a} makes use of fuzzy c-means clustering and builds stochastic models along with artificial neural networks to achieve the same objectives. Finally, hierarchical clustering paired with classification techniques is also useful, as shown in \cite{Wang2018}. \\
In this work we aim to use only clustering techniques to analyze a dataset containing some fraudulent transactions and the features associated with them. These clustering techniques will be analyzed in order to get some insights about the data and the costumers.

\section{Methodology}

\subsection{Data}
\begin{enumerate}
    \item \textit{Toy dataset: }The initial test dataset for the algorithms was the iris dataset, that contains 3 classes of iris flowers (versicolor, virginica and setosa) and has 4 features (sepal length and height and petal length and width) with 150 samples.
    \item \textit{Credit card dataset: } This dataset was downloaded from Kaggle (\url{https://www.kaggle.com/mlg-ulb/creditcardfraud}) and contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172\% of all transactions.
\end{enumerate}

\subsection{Pipeline}
\begin{enumerate}
    \item \textit{Preprocessing: }Elimination of missing values and normalization. The normalization method performed is dividing by the maximum of each feature. This to ensure that all the data points are mapped into the hypercube [0,1].
    \item \textit{Statistical analysis:} In order to asses the complexity of the problem, several statistical tests were taken, including normality tests, Independence tests, dsitribution tests and stationarity tests. 
    \item \textit{Feature selection and extraction: }This step was performed differently in both datasets, the toy dataset and the credit card dataset. In the first one, 4 characteristics were extracted, augmenting the dimension from 4 to 8. In the second one, 28 characteristics were provided, and a reduction of dimensionality via PCA was performed.
    \item \textit{Embbeding: }The T-distributed Stochastic Neighbor Embedding was used as embbeding algorithm, due to its distance conserving property. This embbeding was used to visualize the results and also to learn in lower dimensions.
    \item \textit{Learning: }5 clustering algorithms were implemented in both datasets, performing the learning task in the higher, medium and lower dimensional space and comparing the results.
\end{enumerate}
\subsection{Algorithms}
Let $\{x_1,\dots,x_n\}$ be $n$ data points in an $M$ dimensional space, normalized to an hypercube. Also, let $d(x,y)$ be a distance function in $\mathbb{R}^n$.
\begin{enumerate}
    \item \textit{Subtractive clustering:} This method takes all the points as cluster centers candidates. The steps for the algorithm are:
    \begin{enumerate}
        \item Calculate a density function for each pint $x_i$, given by:
    \begin{equation*}
        D_i = \sum_{i=1}^n \exp\left(-\frac{d(x_i,x_j)^2}{(r_a/2)^2}\right)
    \end{equation*}
    where $r_a$ is a positive constant representing a neighborhood radius. 
    \item Choose the cluster center $x_{ci}$ as the point having the largest density value $D_{c_i}$. 
    \item Revise each point's density function by eliminating the effect of the previously chosen center as follows:
\begin{equation*}
    D_i = D_i - D_{c_i}\exp\left(-\frac{d(x_i,x_{c_i})^2}{(r_b/2)^2}\right)
\end{equation*}
where $r_b$ is a positive constant which defines a neighborhood that has measurable reductions in density measure.
\item If a sufficient number of clusters is reached, or the value of the density function is too small, stop. If not, go back to step 2.
    \end{enumerate}
    \item \textit{Mountain clustering:}
    This method, instead of taking each point as a cluster candidate, it forms a grid in the data space, where the intersections of the grid lines represent the potential cluster centers. These points are stored in set $V$. Then, we apply the following algorithm:
    \begin{enumerate}
        \item Calculate a mountain function for each point $v_j$ in $V$, given by:
    \begin{equation*}
        m_j = \sum_{i=1}^n \exp\left(-\frac{d(v_j-x_i)^2}{2\sigma^2}\right)
    \end{equation*}
    where $\sigma$ is a positive constant representing the height and the smoothness of the mountain. 
    \item Choose the cluster center $v_{ci}$ as the point having the largest mountain value $m_{c_i}$. 
    \item Revise each point's mountain function by eliminating the effect of the previously chosen center as follows:
\begin{equation*}
    m_j = m_j - m_{c_i}\exp\left(-\frac{d(v_j-v_{c_i})^2}{(2\beta^2}\right)
\end{equation*}
where $\beta$ is a positive constant.
\item If a sufficient number of clusters is reached, or the value of the mountain function is too small, stop. If not, go back to step 2.
    \end{enumerate}
    \item \textit{k-means clustering} In this algorithm, the dataset is going to be partitioned in $k$ groups $G_i$, $i = 1,\dots,k$. The cost function, based on a distance $d(x,y)$, can be defined by:
    \begin{equation*}
        J = \sum_{i=1}^c J_i = \sum_{i=1}^c\left(\sum_{k,x_k\in G_i} d(x_k,c_i)^2\right)
    \end{equation*}
    For the development, we follow these steps:
    \begin{enumerate}
        \item Randomly initialize the cluster centers $c_i$, $i = 1,\dots, k$.
        \item Determine a membership matrix for each group, $U$ by:
        \begin{equation*}
            u_{ij} = \left\lbrace\begin{array}{ll}
            1     & ||x_j - c_i||^2 \leq ||x_j - c_k||^2 \qquad \forall k\neq i \\
             0    & \text{ Otherwise} 
            \end{array}\right.
        \end{equation*}
        \item Compute the cost function. Stop if it is bellow a certain tolerance $\varepsilon$ or its improvement is irrelevant.
        \item Update the cluster centers by:
        \begin{equation*}
            c_i = \frac{1}{|G_i|}\sum_{k, x_k\in G_i}x_k
        \end{equation*}
        Then, go to the second step.
    \end{enumerate}
    \item \textit{Fuzzy c-means clustering}
    In this algorithm, each point of the dataset belongs to each of the $c$ clusters in a certain degree of membership, between 0 and 1. The steps to follow are:
    \begin{enumerate}
        \item Initialize the membership matrix $U$ with random values between 0 and 1 such that:
        \begin{equation*}
            \sum_{i=1}^c u_{ij} = 1 \qquad \forall j=1,\dots, n
        \end{equation*}
        \item Calculate $c$ fuzzy cluster centers $c_i$, $i = 1,\dots,n$ by:
        \begin{equation*}
            c_i = \frac{\sum_{j=1}^n u_{ij}^m x_j}{\sum_{j=1}^n u_{ij}^m}
        \end{equation*}
        \item Compute the cost function:
        \begin{equation*}
            J(U,c_1,\dots,c_c) = \sum_{i=1}^cJ_i = \sum_{i=1}^c\sum_{j=1}^n u_{ij}^md_{ij}^2
        \end{equation*}
        where $u_{ij}$ is between 0 and 1, $c_i$ is the cluster center of fuzzy group $i$, $d_{ij} = d(c_i,x_j)$ and $m\in[1,\infty)$ is a weighting exponent. Stop if it is either bellow a certain tolerance $\varepsilon$ or if there is no improvement.
        \item Compute a new membership matrix using the following:
        \begin{equation*}
            u_{ij} = \frac{1}{\sum_{k=1}^c\left(\frac{d_{ij}}{d_{kj}}\right)^{\frac{2}{m-1}}}
        \end{equation*}
        Then, go to step 2.
    \end{enumerate}
    \item \textit{K-medians clustering} This is a variation of the k-means clustering \cite{median} in which, instead of computing the mean of the points in a cluster, one calculates the median. This algorithm is more robust to outliers in the dataset.
\end{enumerate}
\subsection{Distance functions}
As defined above, all of the mentioned algorithms calculate distances between points. Let $x = (x_1,\dots,x_n)$ and $y = (y_1,\dots,y_n)$ be two points in $\mathbb{}^n$. The distance functions that are going to be used are the following:
\begin{itemize}
    \item \textit{Euclidean distance}
    \begin{equation*}
        d(x,y) = \sqrt{\sum_{i=1}^n (x_i-y_i)^2}
    \end{equation*}
    \item \textit{Manhattan distance}
    \begin{equation*}
       d(x,y)  = \sum_{i=1}^n |x_i-y_i|
    \end{equation*}
    \item \textit{Cosine similarity}
    \begin{equation*}
        d(x,y) = \frac{\sum_{i=1}^n x_iy_i}{\sum_{i=1}^n x_i^2\sum_{i=1}^n y_i^2}
    \end{equation*}
\end{itemize}
\subsection{Validation}
Both datasets contain groundtruths for the classes of each sample. The previously presented algorithms will be evaluated using some indices (intra cluster and extra cluster) with both of the datasets. Let $c_i$, $i = 1,\dots, k$ be the cluster centers and $C_i$ be the sets containing the points of each cluster. The indices used are described in \cite{validation} and include:
\begin{itemize}
    \item \textit{Davies-Bouldin (DB) index} This index us a function of the \textit{within cluster scatter} and the \textit{between cluster separation}. We define:
    \begin{enumerate}
        \item Within cluster scatter: $S_i = \displaystyle{\frac{1}{|C_i|}\sum_{x\in C_i}||x - z_i||}$.
        \item Between cluster separation: Is simply the distance $d_{ij}$ between two cluster centers $||c_i-c_j||$.
        \item $\displaystyle{R_{i,qt} = \max\limits_{j,j\neq i}\left\lbrace\frac{S_{i,q} + S_{j+q}}{d_{ij}t}\right\rbrace}$
    \end{enumerate}
    And the index is defined as:
    \begin{equation*}
        DB = \frac{1}{k}\sum_{i=1}^k R_{i,qt}
    \end{equation*}
    The objective is to minimize this index for proper clustering.
    \item \textit{Calinski Harabasz (CH) Index} Let $c$ be the centroid of the entire dataset. The index is defined as
    \begin{equation*}
        CH = \frac{ \left[\frac{\sum_{l = 1}^k |C_l|||c_l - c||^2}{k-1}\right]}{\left[\frac{\sum_{l=1}^k \sum_{i=1}^{|C_l}||x_i-c_l||^2}{n-k}\right]}
    \end{equation*}
\end{itemize}

\section{Results}
\subsection{Iris dataset}

\begin{itemize}
    \item \textbf{Subtractive clustering}\\
    The iris dataset contains a ground truth of 3 clusters. In Tables \ref{tab:i1_n_s}, \ref{tab:i2_n_s} and \ref{tab:ie_n_s} are described the number of clusters found by the subtractive algorithm with various configurations of neighborhood ratios $r_a$ and metrics. The Figures \ref{fig:sub-i-iris} and \ref{fig:sub-f-iris} show the initial state of the density function and the final state for one configuration ($r_a =0.5$, metric euclidean),respectively.
    \begin{table}[ht!]
        \centering
        \begin{tabular}{lrrr}
        \toprule
        $r_a$ &  euclidean &  cosine &  cityblock \\
        \midrule
        0.4 &          4 &       1 &          6 \\
        0.5 &          3 &       1 &          4 \\
        0.7 &          2 &       1 &          3 \\
        \bottomrule \\
\end{tabular}
        \caption{Number of clusters given by the subtractive cluster for the iris dataset.}
        \label{tab:i1_n_s}
    \end{table}
    \begin{table}[ht!]
        \centering
        \begin{tabular}{lrrr}
        \toprule
        $r_a$ &  euclidean &  cosine &  cityblock \\
        \midrule
        0.4 &         99 &       4 &         99 \\
        0.5 &         93 &       4 &         99 \\
        0.7 &          8 &       3 &         94 \\
        \bottomrule \\
        \end{tabular}
        \caption{Number of clusters given by the subtractive cluster for the augmented iris dataset.}
        \label{tab:i2_n_s}
    \end{table}
    
    \begin{table}[ht!]
        \centering
        \begin{tabular}{lrrr}
        \toprule
        $r_a$ &  euclidean &  cosine &  cityblock \\
        \midrule
        0.4 &         99 &       3 &         99 \\
        0.5 &         91 &       2 &         99 \\
        0.7 &         37 &       2 &         48 \\
        \bottomrule \\
        \end{tabular}
        \caption{Number of clusters given by the subtractive cluster for the embbeded iris dataset.}
        \label{tab:ie_n_s}
    \end{table}
    Despite the fact that the dataset provides clearly at least 2 groups, the results vary among the different parameters for the subtractive algorithm. We observe that, for the embbeded data, the cosine norm is usually better than the others, as well as for the augmented dataset. But the euclidean norm works better for the original dataset, as shown in Figure~\ref{fig:sub-iris}.
    \begin{table}[ht!]
        \centering
        \begin{tabular}{lrrr}
        \toprule
        $r_a$ &   euclidean &  cosine &   cityblock \\
        \midrule
        0.4 &  523.017618 &     NaN &  415.515675 \\
        0.5 &  603.592099 &     NaN &  522.011796 \\
        0.7 &  497.999891 &     NaN &  594.486176 \\
        \bottomrule \\
        \end{tabular}
        \caption{Calinski-Harabasz score for the iris dataset clustered with the subtractive algorithm.}
        \label{tab:i1_ch}
    \end{table}
    \begin{table}[ht!]
        \centering
        \begin{tabular}{lrrr}
        \toprule
        $r_a$ &  euclidean &  cosine &  cityblock \\
        \midrule
        0.4 &   0.800206 &     NaN &   1.054853 \\
        0.5 &   0.634911 &     NaN &   0.817205 \\
        0.7 &   0.390944 &     NaN &   0.640144 \\
        \bottomrule \\
        \end{tabular}
        \caption{Davies-Bouldin index for the iris dataset clustered with the subtractive algorithm.}
        \label{tab:i1_db}
    \end{table}
    In Tables \ref{tab:i1_ch} and \ref{tab:i1_db} we observe two indices for the clustering performed in the original dataset. Note that, for the cosine norm the number of clusters achieved is 1, and the metric is not defined for less than 2 groups. We can observe that the highest score and the lowest index are achieved with the euclidean norm, as we stated in the results for the number of clusters.
    \begin{table}[ht!]
        \centering
        \begin{tabular}{lrrr}
        \toprule
        $r_a$ &     euclidean &    cosine &     cityblock \\
        \midrule
        0.4 &  76625.576579 &  3.018177 &  78208.081628 \\
        0.5 &  53652.621959 &  3.874866 &  74559.857679 \\
        0.7 &      2.602149 &  4.132075 &  55651.428403 \\
        \bottomrule \\
        \end{tabular}
        \caption{Calinski-Harabasz score for the augmented iris dataset clustered with the subtractive algorithm.}
        \label{tab:i2_cb}
    \end{table}
    \begin{table}[ht!]
        \centering
        \begin{tabular}{lrrr}
        \toprule
        $r_a$ &  euclidean &    cosine &  cityblock \\
        \midrule
        0.4 &   0.217657 &  1.903561 &   0.219811 \\
        0.5 &   0.221893 &  1.861367 &   0.228289 \\
        0.7 &   2.739684 &  1.963818 &   0.258687 \\
        \bottomrule \\
        \end{tabular}
        \caption{Davies-Bouldin index for the augmented iris dataset clustered with the subtractive algorithm.}
        \label{tab:i2_db}
    \end{table}
    In Tables \ref{tab:i2_cb} and \ref{tab:i2_db} are shown the results for the validation on the augmented dataset. As seen before, the cosine norm achieves an appropiate number of clusters for this dataset, however, its indices and scores are the worst performed, probably because the other metrics achieve clusters that, even when they are not much informative, separate well the data and overfits the score. \\ 
    \begin{table}[ht!]
        \centering
        \begin{tabular}{lrrr}
        \toprule
        $r_a$ &    euclidean &       cosine &    cityblock \\
        \midrule
        0.4 &  9150.473726 &  2055.146690 &  8762.051647 \\
        0.5 &  6450.492142 &  1626.713464 &  8176.198965 \\
        0.7 &  3367.100226 &  1626.713464 &  3643.894411 \\
        \bottomrule \\
        \end{tabular}
        \caption{Calinski-Harabasz score for the embbeded iris dataset clustered with the subtractive algorithm.}
        \label{tab:ie_ch}
    \end{table}
    \begin{table}[ht!]
        \centering
        \begin{tabular}{lrrr}
        \toprule
        $r_a$ &  euclidean &    cosine &  cityblock \\
        \midrule
        0.4 &   0.270312 &  0.423202 &   0.279279 \\
        0.5 &   0.327560 &  0.216722 &   0.290310 \\
        0.7 &   0.626248 &  0.216722 &   0.564174 \\
        \bottomrule \\
        \end{tabular}
        \caption{Davies-Bouldin index for the embbeded iris dataset clustered with the subtractive algorithm.}
        \label{tab:ie_db}
    \end{table}
    \ \\
    Finally, in Tables \ref{tab:ie_db} and \ref{tab:ie_ch} we observe the valdiation results for the embbeded dataset (that contained only 2 features). Again, as for the augmented dataset, the euclidean and cityblock metrics are performing better, but we observe that is probably because the number of clusters achieved is huge.
    \begin{figure}[ht]
    \centering
    \includegraphics[scale = 0.45]{figures/iris/subtractive-i.png}
    \caption{Initial density function for the subtractive clustering using the euclidean norm.}
    \label{fig:sub-i-iris}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[scale = 0.5]{figures/iris/subtractive-f.png}
    \caption{Final density function for the subtractive clustering using the euclidean norm.}
    \label{fig:sub-f-iris}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[scale = 0.5]{figures/iris/subtractive.png}
    \caption{Clustering for the iris dataset using the subtractive algorithm.}
    \label{fig:sub-iris}
\end{figure}

    \item \textbf{Mountain clustering:}
     In Tables \ref{tab:i1_n_m}, \ref{tab:i2_n_m} and \ref{tab:ie_n_m} are described the number of clusters found by the mountain algorithm with various configurations of smoothing parameters $\sigma$ and metrics. For $\sigma = 1.5$ and the euclidean metric, the first and last mountain function is shown in Figures \ref{fig:mon-i-iris} and \ref{fig:sub-f-iris}, respectively.
    \begin{table}[ht!]
        \centering
        \begin{tabular}{lrrr}
            \toprule
            $\sigma$ &  euclidean &  cosine &  cityblock \\
            \midrule
            0.4 &          5 &       3 &          7 \\
            0.5 &          3 &       2 &          4 \\
            0.7 &          2 &       1 &          3 \\
            \bottomrule \\
        \end{tabular}
        \caption{Number of clusters found by the mountain algorithm in the iris dataset.}
        \label{tab:i1_n_m}
    \end{table}
    
    \begin{table}[ht!]
        \centering
        \begin{tabular}{lrrr}
        \toprule
        $\sigma$ &  euclidean &  cosine &  cityblock \\
        \midrule
        0.4 &          4 &       4 &          2 \\
        0.5 &          3 &       4 &          2 \\
        0.7 &          2 &       2 &          2 \\
        \bottomrule \\
        \end{tabular}
        \caption{Number of clusters found by the mountain algorithm in the augmented iris dataset.}
        \label{tab:i2_n_m}
    \end{table}
    
    \begin{table}[ht!]
        \centering
        \begin{tabular}{lrrr}
        \toprule
        $\sigma$ &  euclidean &  cosine &  cityblock \\
        \midrule
        0.4 &          2 &       3 &          2 \\
        0.5 &          2 &       3 &          2 \\
        0.7 &          3 &       2 &          3 \\
        \bottomrule \\
        \end{tabular}
        \caption{Number of clusters found by the mountain algorithm in the embbeded iris dataset.}
        \label{tab:ie_n_m}
    \end{table}
    In comparison with the subtractive algorithm, the mountain algorithm is able to approximate better the number of clusters existing in the iris dataset, for any configuration of the smoothing parameter $\sigma$ and metric. However, as shown in the results of Figure~\ref{fig:mon-iris}, some configurations are better than others in finding the correct amount of clusters.
    \begin{table}[ht!]
        \centering
        \begin{tabular}{lrrr}
        \toprule
        $\sigma$ &   euclidean &  cosine &   cityblock \\
        \midrule
        0.4 &  392.333739 &     NaN &  245.874313 \\
        0.5 &  586.577476 &     NaN &  419.084383 \\
        0.7 &  497.999891 &     NaN &  567.385797 \\
        \bottomrule \\
        \end{tabular}
        \caption{Calinski-Harabasz score for the iris dataset clusterd by the mountain algorithm.}
        \label{tab:i1_ch_m}
    \end{table}
    
    \begin{table}[ht!]
        \centering
        \begin{tabular}{lrrr}
        \toprule
        $\sigma$ &  euclidean &  cosine &  cityblock \\
        \midrule
        0.4 &   0.803257 &     NaN &   1.185689 \\
        0.5 &   0.641318 &     NaN &   0.678465 \\
        0.7 &   0.390944 &     NaN &   0.662073 \\
        \bottomrule \\
        \end{tabular}
        \caption{Davis Bouldin index for the iris dataset clusterd by the mountain algorithm.}
        \label{tab:i1_db_m}
    \end{table}
    
    In Tables \ref{tab:i1_ch_m} and \ref{tab:i1_db_m} are shown the validity indices for the original iris dataset. We can see that, despite the fact that the mountain with the cosine metric finds more than 1 cluster, the centroids of some of them are so far away fro the data that one ends up having only one cluster, and that is why the indices are not defined in that point. We observe that, in this case, the euclidean metric gives the best overall performance. \\
    
    \begin{table}[ht!]
        \centering
        \begin{tabular}{lrrr}
        \toprule
        $\sigma$ &  euclidean &    cosine &  cityblock \\
        \midrule
        0.4 &   0.807805 &  4.606519 &   0.299737 \\
        0.5 &   1.235066 &  1.092758 &   0.299737 \\
        0.7 &   0.860060 &  1.378751 &   0.299737 \\
        \bottomrule \\
        \end{tabular}
        \caption{Calinski-Harabasz score for the augmented iris dataset clustered by the mountain algorithm.}
        \label{tab:i2_ch_m}
    \end{table}
    
    \begin{table}[ht!]
        \centering
        \begin{tabular}{lrrr}
        \toprule
        $\sigma$ &  euclidean &    cosine &  cityblock \\
        \midrule
        0.4 &   4.605520 &  2.506750 &   7.163532 \\
        0.5 &  10.291972 &  2.595677 &   7.163532 \\
        0.7 &   4.561892 &  2.061052 &   7.163532 \\
        \bottomrule \\
        \end{tabular}
        \caption{Davis Bouldin index for the augmented iris dataset clustered by the mountain algorithm.}
        \label{tab:i2_db_m}
    \end{table}
    
    In Tables \ref{tab:i2_ch_m} and \ref{tab:i2_db_m} are shown the results for the validation of the mountain algorithm in the augmented dataset. In this case, we have always more than 1 cluster, which is a good result. Here, the cityblock (Manhattan) distance shows the best performance overall. \\
    
    \begin{table}[ht!]
        \centering
        \begin{tabular}{lrrr}
        \toprule
        $\sigma$ &   euclidean &       cosine &   cityblock \\
        \midrule
        0.4 &   61.900352 &  1400.846404 &   62.435134 \\
        0.5 &   61.900352 &  1400.846404 &   62.435134 \\
        0.7 &  106.215548 &  1626.713464 &  104.916325 \\
        \bottomrule \\
        \end{tabular}

        \caption{Calinski-Harabasz score for the embbeded iris dataset clustered by the mountain algorithm.}
        \label{tab:ie_ch_m}
    \end{table}
    
    \begin{table}[ht!]
        \centering
        \begin{tabular}{lrrr}
        \toprule
        $\sigma$ &  euclidean &    cosine &  cityblock \\
        \midrule
        0.4 &   0.820981 &  0.522071 &   0.833421 \\
        0.5 &   0.820981 &  0.522071 &   0.833421 \\
        0.7 &   0.685696 &  0.216722 &   0.686128 \\
        \bottomrule \\
        \end{tabular}
        \caption{Davis Bouldin index for the embbeded iris dataset clustered by the mountain algorithm.}
        \label{tab:ie_db_m}
    \end{table}
    
    Finally, in Tables \ref{tab:ie_ch_m} and \ref{tab:ie_db_m} are shown the validity indices for the embbeded dataset. Again, we have more than 1 cluster and the manhattan and euclidean distance show a similar performance overall, being both better than the cosine distance.
\end{itemize}



\begin{figure}[ht]
    \centering
    \includegraphics[scale = 0.45]{figures/iris/mountain-i.png}
    \caption{Initial mountain function for the mountain clustering using the euclidean norm.}
    \label{fig:mon-i-iris}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[scale = 0.45]{figures/iris/mountain-f.png}
    \caption{Final mountain function for the mountain clustering using the euclidean norm.}
    \label{fig:mon-f-iris}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[scale = 0.45]{figures/iris/mountain.png}
    \caption{Clustering for the iris dataset using the mountain algorithm.}
    \label{fig:mon-iris}
\end{figure}

\subsubsection{k-means clustering}
For this algorithm, we used as a parameter $k$, number of clusters found by the subtractive and mountain clustering algorithm. Also, we took into account the different metrics proposed and calculated the validity indices for each one of the datasets.\\

\begin{table}[ht!]
    \centering
    \begin{tabular}{lrrr}
\toprule
k &  euclidean &    cosine &  cityblock \\
\midrule
2 &   0.390944 &  0.390944 &   0.432572 \\
3 &   0.627713 &  0.768128 &   0.631299 \\
5 &   1.050796 &  1.068637 &   1.073839 \\
7 &   1.090907 &  1.023752 &   1.093764 \\
\bottomrule \\
\end{tabular}
    \caption{Davis Bouldin index for the  iris dataset clustered by the k-means.}
        \label{tab:i1_db_km}
\end{table}

\begin{table}[ht!]
    \centering
    \begin{tabular}{lrrr}
\toprule
k &   euclidean &      cosine &   cityblock \\
\midrule
2 &  497.999891 &  497.999891 &  480.870089 \\
3 &  621.170676 &  261.714855 &  615.448293 \\
5 &  378.906136 &  394.653049 &  400.539228 \\
7 &  343.190723 &  296.886143 &  312.518317 \\
\bottomrule \\
\end{tabular}
   \caption{Calinski Harabasz index for the  iris dataset clustered by the k-means.}
    \label{tab:i1_ch_km}
\end{table}

In Tables \ref{tab:i1_db_km} and \ref{tab:i1_ch_km} are shown the validity indices for the iris dataset using different number of cluster centers and the k-means algorithm. As one would expect 

\begin{table}[ht!]
    \centering
   \begin{tabular}{lrrr}
    \toprule
    k &  euclidean &    cosine &  cityblock \\
    \midrule
    2 &   0.446561 &  2.050121 &   0.446561 \\
    3 &   0.692282 &  1.978282 &   1.123834 \\
    5 &   0.677904 &  1.699866 &   0.872046 \\
    7 &   0.830947 &  2.634747 &   0.942682 \\
    \bottomrule \\
    \end{tabular}

    \caption{Davis Bouldin index for the augmented iris dataset clustered by the k-means.}
    \label{tab:i2_db_km}
\end{table}

\begin{table}[ht!]
    \centering
    \begin{tabular}{lrrr}
    \toprule
    k &   euclidean &    cosine &   cityblock \\
    \midrule
    2 &  538.658115 &  3.665251 &  538.658115 \\
    3 &  274.139485 &  2.777438 &  273.314739 \\
    5 &  122.328462 &  3.158810 &  121.696067 \\
    7 &   72.020897 &  4.419832 &   71.318473 \\
    \bottomrule\\
    \end{tabular}
    \caption{Calinski Harabasz index for the augmented iris dataset clustered by the k-means.}
    \label{tab:i2_ch_km}
\end{table}

\begin{table}[ht!]
    \centering
    \begin{tabular}{lrrr}
    \toprule
    k &  euclidean &    cosine &  cityblock \\
    \midrule
    2 &   0.216722 &  0.216722 &   0.216722 \\
    3 &   0.388003 &  0.423202 &   0.389462 \\
    5 &   0.733313 &  0.806583 &   0.716881 \\
    7 &   0.815055 &  1.537590 &   0.710355 \\
    \bottomrule \\
    \end{tabular}
    \caption{Davis Bouldin index for the embbeded iris dataset clustered by the k-means.}
    \label{tab:i2e_db_km}
\end{table}

\begin{table}[ht!]
    \centering
    \begin{tabular}{lrrr}
    \toprule
    k &    euclidean &       cosine &    cityblock \\
    \midrule
    2 &  1626.713464 &  1626.713464 &  1626.713464 \\
    3 &  2889.067753 &  2055.146690 &  2865.014582 \\
    5 &  2007.575346 &  1598.184855 &  3033.948243 \\
    7 &  2752.850581 &  1104.080592 &  2367.108834 \\
    \bottomrule\\
    \end{tabular}
\caption{Calinski Harabasz index for the embbeded iris dataset clustered by the k-means.}
    \label{tab:ie_ch_km}
\end{table}

\subsubsection{c-means clustering}

\begin{table}[ht!]
    \centering
    \begin{tabular}{lrrr}
\toprule
c &  euclidean &    cosine &  cityblock \\
\midrule
2 &   0.390944 &  0.390944 &   0.432572 \\
3 &   0.627713 &  0.723240 &   0.634940 \\
5 &   1.250634 &  2.024434 &   1.116347 \\
7 &   1.242524 &  1.768239 &   2.299770 \\
\bottomrule \\
\end{tabular}
    \caption{Davies Bouldin index for the iris dataset clustered by the c-means algorithms}
    \label{tab:i1_db_c}
\end{table}

\begin{table}[ht!]
    \centering
    \begin{tabular}{lrrr}
\toprule
c &   euclidean &      cosine &   cityblock \\
\midrule
2 &  497.999891 &  497.999891 &  480.870089 \\
3 &  621.170676 &  503.444361 &  610.008589 \\
5 &  382.886459 &  218.393685 &  398.329348 \\
7 &  306.272110 &  138.972782 &  255.833890 \\
\bottomrule \\
\end{tabular}
    \caption{Calinski-Harabasz score for the iris dataset clustered by the c-means algorithm.}
    \label{tab:i1_ch_c}
\end{table}

\begin{table}[ht!]
    \centering
    \begin{tabular}{lrrr}
\toprule
c &  euclidean &    cosine &  cityblock \\
\midrule
2 &   0.446561 &  4.206418 &   0.633145 \\
3 &   0.615052 &  2.110445 &   0.633145 \\
5 &   0.857268 &  4.773747 &   1.016935 \\
7 &   1.448554 &  1.605220 &   1.401289 \\
\bottomrule \\
\end{tabular}
    \caption{Davies Bouldin index for the augmented iris dataset clustered by the c-means algorithms}
    \label{tab:i2_db_c}
\end{table}

\begin{table}[ht!]
    \centering
    \begin{tabular}{lrrr}
\toprule
c &   euclidean &     cosine &   cityblock \\
\midrule
2 &  538.658115 &   0.895073 &  291.212797 \\
3 &  354.137296 &   5.245520 &  291.212797 \\
5 &  221.695808 &   4.462668 &  217.618208 \\
7 &  178.374197 &  13.508430 &  178.359676 \\
\bottomrule \\
\end{tabular}
    \caption{Calinski-Harabasz score for the augmented iris dataset clustered by the c-means algorithm.}
    \label{tab:i2_ch_c}
\end{table}

\begin{table}[ht!]
    \centering
    \begin{tabular}{lrrr}
\toprule
c &  euclidean &    cosine &  cityblock \\
\midrule
2 &   0.216722 &  0.216722 &   0.216722 \\
3 &   0.490804 &  0.421414 &   0.389462 \\
5 &   0.725335 &  0.797240 &   0.739533 \\
7 &   0.980847 &  1.246126 &   0.814877 \\
\bottomrule \\
\end{tabular}
    \caption{Davies Bouldin index for the embbeded iris dataset clustered by the c-means algorithms}
    \label{tab:ie_db_c}
\end{table}

\begin{table}[ht!]
    \centering
    \begin{tabular}{lrrr}
\toprule
c &    euclidean &       cosine &    cityblock \\
\midrule
2 &  1626.713464 &  1626.713464 &  1626.713464 \\
3 &   859.503927 &  2336.157328 &  1626.713464 \\
5 &  2983.511715 &  1733.911811 &  2870.216951 \\
7 &  2694.962314 &  1278.579394 &  2809.017173 \\
\bottomrule \\
\end{tabular}
    \caption{Calinski-Harabasz score for the augmented iris dataset clustered by the c-means algorithm.}
    \label{tab:ie_ch_c}
\end{table}

\subsubsection{k-medians clustering}

\begin{table}[ht!]
    \centering
    \begin{tabular}{lrrr}
        \toprule
        k &  euclidean &    cosine &  cityblock \\
        \midrule
        2 &   0.390944 &  0.390944 &   0.390944 \\
        3 &   0.723271 &  0.877327 &   0.635719 \\
        5 &   1.001479 &  1.773590 &   1.059862 \\
        7 &   1.243207 &  2.032079 &   1.211970 \\
        \bottomrule\\
        \end{tabular}
    \caption{Davies Bouldin index for the iris dataset clustered by the k-medians algorithm.}
    \label{tab:i1_db_k}
\end{table}

\begin{table}[ht!]
    \centering
   \begin{tabular}{lrrr}
    \toprule
    k &   euclidean &      cosine &   cityblock \\
    \midrule
    2 &  497.999891 &  497.999891 &  497.999891 \\
    3 &  621.170676 &  387.292106 &  607.470606 \\
    5 &  396.983499 &  206.982144 &  343.632905 \\
    7 &  243.888933 &  129.490532 &  264.817439 \\
    \bottomrule \\
    \end{tabular}
    \caption{Calinski Harabsz score for the iris dataset clustered by the k-medians algorithm.}
    \label{tab:i1_ch_k}
    \end{table}
    
    \begin{table}[ht!]
        \centering
        \begin{tabular}{lrrr}
        \toprule
        k &  euclidean &    cosine &  cityblock \\
        \midrule
        2 &   2.030315 &  2.050757 &   1.663545 \\
        3 &   1.851582 &  1.932578 &   1.710490 \\
        5 &   1.246690 &  2.337498 &   3.811988 \\
        7 &   1.551410 &  1.689764 &   1.417060 \\
        \bottomrule
        \end{tabular}
        \caption{Davies Bouldin index for the augmented iris dataset clustered by the k-medians algorithm.}
    \label{tab:i2_db_k}
    \end{table}


\begin{table}[ht!]
    \centering
    \begin{tabular}{lrrr}
    \toprule
    k &  euclidean &    cosine &  cityblock \\
    \midrule
    2 &   7.494573 &  2.094386 &   1.077398 \\
    3 &   1.176475 &  4.798218 &   1.004791 \\
    5 &  91.956606 &  2.520011 &   1.035365 \\
    7 &  54.780219 &  1.790446 &   8.107000 \\
    \bottomrule \\
    \end{tabular}
    \caption{Calinski Harabsz score for the iris augmented dataset clustered by the k-medians algorithm}
    \label{tab:i2_ch_k}
\end{table}

\begin{table}[ht!]
    \centering
    \begin{tabular}{lrrr}
    \toprule
    k &  euclidean &    cosine &  cityblock \\
    \midrule
    2 &   0.216722 &  0.216722 &   0.216722 \\
    3 &   0.503031 &  0.393918 &   0.390287 \\
    5 &   0.730190 &  1.101565 &   0.690618 \\
    7 &   0.797229 &  1.125930 &   0.872588 \\
    \bottomrule \\
    \end{tabular}
    \caption{Davies Bouldin index for the embbeded iris dataset clustered by the k-medians algorithm.}
    \label{tab:ie_db_k}
\end{table}

\begin{table}[ht!]
    \centering
    \begin{tabular}{lrrr}
    \toprule
    k &    euclidean &       cosine &    cityblock \\
    \midrule
    2 &  1626.713464 &  1626.713464 &  1626.713464 \\
    3 &   859.784900 &  2794.486490 &  2854.605178 \\
    5 &  2991.839844 &  1830.284769 &  1240.660834 \\
    7 &  2744.880380 &  1052.571840 &  2337.767527 \\
    \bottomrule \\
    \end{tabular}

    \caption{Calinski Harabsz score for the iris embbeded dataset clustered by the k-medians algorithm}
    \label{tab:ie_ch_k}
\end{table}

\subsubsection{Comparison of the methods}
In Figures~\ref{fig:k2}, \ref{fig:k3}, \ref{fig:k4} and \ref{fig:k7} are shown the different centers provided by the different algorithms explored in this work, for the iris dataset.
\begin{figure}[ht!]
    \centering
    \includegraphics[scale = 0.4]{figures/iris/k2.png}
    \caption{Clusters given by different methods, $k = 2$.}
    \label{fig:k2}
\end{figure}

\begin{figure}[ht!]
    \centering
    \includegraphics[scale = 0.4]{figures/iris/k3.png}
    \caption{Clusters given by different methods, $k = 3$.}
    \label{fig:k3}
\end{figure}

\begin{figure}[ht!]
    \centering
    \includegraphics[scale = 0.4]{figures/iris/k4.png}
    \caption{Clusters given by different methods, $k = 4$.}
    \label{fig:k4}
\end{figure}

\begin{figure}[ht!]
    \centering
    \includegraphics[scale = 0.4]{figures/iris/k7.png}
    \caption{Clusters given by different methods, $k = 7$.}
    \label{fig:k7}
\end{figure}

Is clear that, even when the number of centers are the same, they vary in terms of localization, sometimes more than others. The three algorithms that need the number of centers tend to give more similar ones, while the ones that explore the whole space differ more in this particular dataset.
\input{credit}

\section{Conclusions}

\nocite{*}
\bibliography{ref}
\bibliographystyle{IEEEtran}


\end{document}
